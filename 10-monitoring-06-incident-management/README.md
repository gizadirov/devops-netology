# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Основная часть

Составьте постмортем на основе реального сбоя системы GitHub в  году.

Информация о сбое: 

* [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/);
* [развёрнуто на английском языке](https://github.blog/-10-30-oct21-post-incident-analysis/).

## Ответ

| <!-- -->                    |<!-- -->                                                   |
|-----------------------------|-----------------------------------------------------------|
| Краткое описание инцидента  | После плановой потери соединения между датацентрами восточного и западного побережья, распался кластер, некоторое время базы данных были в несогласованном состоянии, после восстановления соединения репликация не завершилась успехом. Вследствие чего сайт перестал выдавать корректные, согласованные данные. Для восстановления пришлось ограничить фунцкционал webhook-ов и Github Pages. Инцидент привел к ухудшению качества обслуживания на 24 часа и 11 минут.                                                 |
| Предшествующие события      | **21 октября в 22:52 UTC**<br>Плановые работы по техническому обслуживанию по замене вышедшего из строя оптического оборудования 100G привели к потере соединения между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США. Соединение между этими точками было восстановлено за 43 секунды.
| Причина инцидента           | Во время разделения сети, Orchestrator, который был активен в основном центре обработки данных, начал процесс отмены выбора руководства. Центр обработки данных Западного побережья США и узлы Оркестратора общедоступного облака Восточного побережья смогли установить кворум и начать обработку отказа кластеров для направления операций записи в центр обработки данных Западного побережья. Orchestrator приступил к организации топологии кластера базы данных западного побережья. Когда подключение было восстановлено, наш уровень приложений немедленно начал направлять трафик записи на новые основные узлы на сайте западного побережья. Серверы баз данных в центре обработки данных на восточном побережье содержали короткий период записей, которые не были реплицированы на объект на западном побережье. 
| Воздействие                 | Несогласованная информация на сайте
| Обнаружение                 | **21 октября 22:54 UTC**<br>Наши внутренние системы мониторинга начали генерировать оповещения, указывающие на многочисленные сбои в наших системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. К 23:02 UTC инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в неожиданном состоянии. Запрос к API Orchestrator показал топологию репликации базы данных, которая включала только серверы из нашего центра обработки данных на западном побережье США. 
| Реакция                     | **21 октября 23:07 UTC**<br>К этому моменту группа реагирования решила вручную заблокировать наши внутренние инструменты развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. В 23:09 UTC группа реагирования перевела сайт в желтый статус. Это действие автоматически перевело ситуацию в активный инцидент и отправило предупреждение координатору инцидента. В 23:11 UTC к нам присоединился координатор инцидентов и через две минуты изменил статус решения на красный.<br>**21 октября 23:13 UTC**<br>На тот момент стало понятно, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры из команды разработки баз данных GitHub. Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. Эта попытка была сложной, поскольку к этому моменту кластер базы данных Западного побережья принимал записи с нашего уровня приложений в течение почти 40 минут. Кроме того, в кластере Восточного побережья существовало несколько секунд записей, которые не были реплицированы на Западное побережье и препятствовали репликации новых записей обратно на Восточное побережье.<br>**21 октября 23:19 UTC**<br>После запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично снизить удобство использования сайта, приостановив доставку веб-перехватчиков и сборку страниц GitHub вместо того, чтобы поставить под угрозу данные, которые мы уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы поставить целостность данных выше удобства использования сайта и времени восстановления.
| Восстановление              | **22 октября 00:05 UTC**<br>Инженеры, участвующие в группе реагирования на инциденты, начали разработку плана по устранению несоответствий данных и реализации наших процедур аварийного переключения для MySQL. Наш план состоял в том, чтобы восстановиться из резервных копий, синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди. Мы обновили свой статус, чтобы сообщить пользователям, что мы собираемся выполнить контролируемый переход на другой ресурс внутренней системы хранения данных. Хотя резервные копии данных MySQL создаются каждые четыре часа и хранятся в течение многих лет, резервные копии хранятся удаленно в службе хранения BLOB-объектов общедоступного облака. Время, необходимое для восстановления нескольких терабайт резервных данных, привело к тому, что этот процесс занял несколько часов. Значительная часть времени была потрачена на передачу данных из службы удаленного резервного копирования. Процесс распаковки, подсчета контрольной суммы, подготовки и загрузки больших файлов резервных копий на вновь подготовленные серверы MySQL занял большую часть времени. Эта процедура тестируется как минимум ежедневно, поэтому сроки восстановления были хорошо понятны, однако до этого инцидента нам никогда не приходилось полностью восстанавливать весь кластер из резервной копии, и вместо этого мы могли полагаться на другие стратегии, такие как отложенные реплики.<br>**22 октября 00:41 UTC**<br>К этому времени был начат процесс резервного копирования для всех затронутых кластеров MySQL, и инженеры следили за ходом его выполнения. Одновременно несколько групп инженеров искали способы ускорить передачу и восстановление без дальнейшего ухудшения удобства использования сайта или риска повреждения данных.<br>**22 октября 06:51 UTC**<br>Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья. Это приводило к медленной загрузке сайта для страниц, которым приходилось выполнять операцию записи по межстрановой ссылке, но страницы, читающие из этих кластеров баз данных, возвращали актуальные результаты, если запрос на чтение попадал на недавно восстановленную реплику. Другие более крупные кластеры баз данных все еще восстанавливались.<br>Наши команды определили способы восстановления непосредственно с Западного побережья, чтобы преодолеть ограничения пропускной способности, вызванные загрузкой из внешнего хранилища, и были все более уверены в том, что восстановление неизбежно, а время, оставшееся для создания работоспособной топологии репликации, зависело от того, как долго это будет продолжаться. используйте репликацию, чтобы наверстать упущенное. Эта оценка была линейно интерполирована на основе имеющихся у нас телеметрических данных репликации, а страница состояния была обновлена , чтобы установить ожидаемое время восстановления в два часа.<br>**22 октября 07:46 UTC**<br>GitHub опубликовал сообщение в блоге , чтобы предоставить больше контекста. Мы используем GitHub Pages внутри компании, и все сборки были приостановлены несколькими часами ранее, поэтому публикация потребовала дополнительных усилий. Мы приносим извинения за задержку. Мы намеревались разослать это сообщение гораздо раньше и позаботимся о том, чтобы публиковать обновления в будущем с учетом этих ограничений.<br>**22 октября 11:12 UTC**<br>Все первичные базы данных снова установлены на восточном побережье США. В результате сайт стал гораздо более отзывчивым, поскольку записи теперь направлялись на сервер базы данных, который находился в том же физическом центре обработки данных, что и наш уровень приложений. Несмотря на существенное повышение производительности, десятки реплик чтения базы данных по-прежнему отставали от основной на несколько часов. Эти отложенные реплики привели к тому, что пользователи видели противоречивые данные при взаимодействии с нашими сервисами. Мы распределяли нагрузку чтения по большому пулу реплик чтения, и каждый запрос к нашим сервисам имел хорошие шансы попасть в реплику чтения, которая задерживалась на несколько часов.<br>В действительности время, необходимое для догона репликации, подчинялось функции затухания мощности, а не линейной траектории. Из-за увеличения нагрузки на запись в наших кластерах баз данных, когда пользователи просыпались и начинали свой рабочий день в Европе и США, процесс восстановления занял больше времени, чем первоначально предполагалось.<br>**22 октября 13:15 UTC**<br>К этому моменту мы приближались к пиковой нагрузке трафика на GitHub.com. Группа реагирования на инциденты обсудила дальнейшие действия. Было ясно, что задержки репликации увеличивались, а не уменьшались в направлении к согласованному состоянию. Ранее во время инцидента мы начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке Восточного побережья США. Как только они стали доступны, стало проще распределить объем запросов на чтение между большим количеством серверов. Снижение совокупного использования всех реплик чтения позволило репликации наверстать упущенное.<br>**22 октября 16:24 UTC**<br>Как только реплики были синхронизированы, мы выполнили переход на исходную топологию, решая непосредственные проблемы с задержкой и доступностью. В рамках сознательного решения отдать приоритет целостности данных в течение более короткого окна инцидентов, мы сохранили статус службы красным , пока начали обрабатывать накопившиеся данные.<br>**22 октября 16:45 UTC**<br>На этом этапе восстановления нам пришлось сбалансировать возросшую нагрузку, связанную с отставанием, потенциально перегружая наших партнеров по экосистеме уведомлениями, и как можно быстрее вернуть наши услуги на 100%. Было зарегистрировано более пяти миллионов событий-перехватчиков и 80 тысяч сборок страниц в очереди.<br>Когда мы снова включили обработку этих данных, мы обработали около 200 000 полезных нагрузок веб-перехватчиков, которые пережили внутренний срок жизни и были удалены. Обнаружив это, мы приостановили обработку и внесли изменения, чтобы на время увеличить TTL.<br>Чтобы избежать дальнейшего снижения надежности наших обновлений статуса, мы оставались в пониженном статусе до тех пор, пока не завершили обработку всего накопившегося массива данных и не убедились, что наши услуги четко вернулись к нормальному уровню производительности.<br>**22 октября 23:03 UTC**<br>Все ожидающие сборки веб-перехватчиков и страниц были обработаны, а целостность и правильная работа всех систем подтверждены. Статус сайта изменился на зеленый.
| Таймлайн                    | 1. 21 октября 22:52 UTC Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отмены выбора руководства.<br>2. 21 октября 22:54 UTC Наши внутренние системы мониторинга начали генерировать оповещения, указывающие на многочисленные сбои в наших системах.<br>3. 21 октября 23:07 UTC Группа реагирования решила вручную заблокировать наши внутренние инструменты развертывания.<br>4. 21 октября 23:09 UTC Перевод в желтый статус.<br>5. 21 октября 23:11 UTC Перевод в красный статус.<br>6. 21 октября 23:13 UTC Исследование текущго состояния группой доп. инженеров.<br>7. 21 октября 23:19 UTC Приостановка доставки веб-перехватчиков и сборки страниц GitHub.<br>8. 22 октября 00:05 UTC Инженеры начали разработку плана по устранению несоответствий данных и реализации наших процедур аварийного переключения для MySQL.<br>9. 22 октября 00:41 UTC Начат процесс резервного копирования для всех затронутых кластеров MySQL. <br>10. 22 октября 06:51 UTC Несколько кластеров завершили восстановление из резервных копий в центре обработки данных на восточном побережье и начали репликацию новых данных с западного побережья.<br>11. 22 октября 07:46 UTC GitHub опубликовал сообщение в блоге.<br>12. 22 октября 11:12 UTC Все первичные базы данных снова установлены на восточном побережье.<br>13. 22 октября 13:15 UTC Мы приближались к пиковой нагрузке трафика на GitHub.com.<br>14. 22 октября 16:24 UTC Как только реплики были синхронизированы, мы выполнили переход на исходную топологию.<br>15. 22 октября 16:45 UTC Балансировка возросшей нагрузки.<br>16. 22 октября 23:05 UTC Статус сайта изменился на зеленый.   
| Последующие действия        |**Устранение несоответствий данных**<br>Во время восстановления мы записали двоичные журналы MySQL, содержащие записи, выполненные на нашем основном сайте, которые не были реплицированы на наш сайт на западном побережье из каждого затронутого кластера. Общее количество записей, которые не были реплицированы на Западное побережье, было относительно небольшим. Например, в одном из наших самых загруженных кластеров в затронутом окне было 954 записи. В настоящее время мы анализируем эти журналы и определяем, какие записи могут быть автоматически согласованы, а какие потребуют взаимодействия с пользователями. У нас есть несколько команд, участвующих в этой работе, и наш анализ уже определил категорию записей, которые с тех пор были повторены пользователем и успешно сохранились. Как указано в этом анализе, наша основная цель — сохранить целостность и точность данных, которые вы храните на GitHub.<br>**Коммуникация**<br>Желая донести до вас значимую информацию во время инцидента, мы опубликовали несколько публичных оценок времени ремонта, основанных на скорости обработки накопившихся данных. Оглядываясь назад, можно сказать, что наши оценки не учитывали все переменные. Мы сожалеем о возникшей путанице и постараемся предоставить более точную информацию в будущем.<br>**Технические инициативы**<br>В ходе этого анализа был выявлен ряд технических инициатив. Поскольку мы продолжаем проводить обширный внутренний процесс анализа после инцидентов, мы ожидаем определить еще больше работы, которую необходимо выполнить.
---

